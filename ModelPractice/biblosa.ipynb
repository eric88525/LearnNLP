{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/zhaoguangxiang/BiBloSA-pytorch/blob/master/model/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function reference\n",
    "+ https://kknews.cc/zh-tw/code/yegrgnb.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, customize your Linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customizedModule,self).__init__()\n",
    "        \n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout = False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        \n",
    "        # initialize the weight & bias\n",
    "        nn.init.xavier_normal_(c1[0].weight)\n",
    "        nn.init.constant(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))\n",
    "        return c1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build source to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2tSA(customizedModule):\n",
    "    def __init__(self, args, hidden_size):\n",
    "        super(s2tSA, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.s2t_W1 = self.customizedLinear(hidden_size, hidden_size, activation = nn.ReLU())\n",
    "        self.s2t_W = self.customizedLinear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        source2token self-attention module\n",
    "        :param x: (batch, (block_num), seq_len, hidden_size)\n",
    "        :return: s: (batch, (block_num), hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        f = self.s2t_W1(x)\n",
    "        f = self.s2t_W(f)\n",
    "        f = F.softmax(f, dim=-2)\n",
    "        # (batch, (block_num), word_dim)\n",
    "        s = torch.sum(f * x, dim=-2)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4584,  2.0789,  0.5668, -0.6074,  1.6688,  0.0831, -0.0713,  0.1733,\n",
      "          0.2433, -0.3455],\n",
      "        [ 1.1394, -1.6121,  1.3858,  0.0028, -1.2394, -0.0347,  0.9353,  0.1221,\n",
      "         -0.1670,  2.3652],\n",
      "        [ 0.3302, -1.1033, -0.1094, -1.3479,  0.7760, -0.5299, -0.7292,  0.2932,\n",
      "         -0.0578,  0.2370],\n",
      "        [-0.2266, -0.1741, -1.0779, -2.5518, -1.4624, -1.3721, -1.2072,  0.1465,\n",
      "         -0.1631,  0.9194],\n",
      "        [ 0.4715,  1.6250, -0.5335,  1.1893, -0.1670,  0.2464,  0.8059, -2.1331,\n",
      "          1.1223,  1.4092]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\nlu\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2086,  0.2824, -0.5255, -1.1239,  0.0892, -0.4929, -0.0658, -0.1166,\n",
      "         0.0715,  0.6283], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "args = '123'\n",
    "data = torch.randn(5,10)\n",
    "print(data)\n",
    "model = s2tSA(args,10)\n",
    "B = model(data)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build mask block self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mBloSA(customizedModule):\n",
    "    def __init__(self,args,mask):\n",
    "        super(mBloSA,self).__init__()\n",
    "        self.args = args\n",
    "        self.mask = mask\n",
    "        \n",
    "        self.s2tSA = s2tSA(self.args,self.args.word_dim)\n",
    "        self.init_mSA()\n",
    "        self.init_mBloSA()\n",
    "    \n",
    "    def init_mSA(self):\n",
    "        self.m_W1 = self.customizeLinear(self.args.word_dim,self.args.word_dim)\n",
    "        self.m_W2 = self.customizeLinear(self.args.word_dim,self.args.word_dim)\n",
    "        self.m_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "        \n",
    "        self.m_W1[0].bias.requires_grad = False\n",
    "        self.m_W2[0].bias.requires_grad = False\n",
    "        \n",
    "        self.c = nn.Parameter(torch.Tensor([self.args.c],requires_grad=False))\n",
    "        \n",
    "    def init_mBloSA(self):\n",
    "        self.g_w1 = self.customizeLinear(self.args.word_dim,self.args.word_dim)\n",
    "        self.g_w2 = self.customizeLinear(self.args.word_dim,self.args.word_dim)\n",
    "        self.g_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "        \n",
    "        self.g_w1.bias.requires_grad = False\n",
    "        self.g_w2.bias.requires_grad = False\n",
    "        \n",
    "        self.f_w1 = self.customizeLinear(self.args.word_dim*3,self.args.word_dim,activation=nn.ReLU())\n",
    "        self.f_w2 = self.customizeLinear(self.args.word_dim*3,self.args.word_dim)\n",
    "        \n",
    "    def mSA(self,x):\n",
    "        \n",
    "        \"\"\"\n",
    "        masked self-attention module\n",
    "        :param x: (batch, (block_num), seq_len, word_dim)\n",
    "        :return: s: (batch, (block_num), seq_len, word_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # 算有幾個token近來 : n\n",
    "        seq_len = x.size(-2)\n",
    "        \n",
    "        # (batch, (block_num), seq_len, 1, word_dim)\n",
    "        x_i = self.m_W1(x).unsqeeze(-2)\n",
    "        # (batch, (block_num), 1, seq_len, word_dim)\n",
    "        x_j = self.m_W2(x).unsqeeze(-3)\n",
    "        \n",
    "        # triu()是上三角 ， detach()中斷反向傳播\n",
    "        M = Variable(torch.ones((seq_len, seq_len))).cuda(self.args.gpu).triu().detach()\n",
    "        M[M==1] = float('-inf')\n",
    "        \n",
    "        # CASE 1 - x: (batch, seq_len, word_dim)\n",
    "        # (1, seq_len, seq_len, 1)\n",
    "        M = M.contiguous().view(1,M.size(0),M.size(1),1)\n",
    "        \n",
    "        # (batch, 1, seq_len, word_dim)\n",
    "        # padding to deal with nan\n",
    "        pad = torch.zeros(x.size(0),1,x.size(-2),x.size(-1))\n",
    "        pad = Variable(pad).cuda(self.args.gpu).detach()\n",
    "        \n",
    "         # CASE 2 - x: (batch, block_num, seq_len, word_dim)\n",
    "        if len(x.size()) == 4:\n",
    "            M = M.unsqueeze(1)\n",
    "            pad = torch.stack([pad] * x.size(1), dim=1)\n",
    "        \n",
    "        f = self.c * F.tanh((x_i + x_j + self.m_b) /self.c)\n",
    "        \n",
    "        # fw or bw masking\n",
    "        if f.size(-2) > 1:\n",
    "            if self.mask == 'fw':\n",
    "                M = M.transpose(-2, -3)\n",
    "                f = F.softmax((f + M).narrow(-3, 0, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([f, pad], dim=-3)\n",
    "            elif self.mask == 'bw':\n",
    "                f = F.softmax((f + M).narrow(-3, 1, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([pad, f], dim=-3)\n",
    "            else:\n",
    "                raise NotImplementedError('only fw or bw mask is allowed!')\n",
    "        else:\n",
    "            f = pad\n",
    "            \n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        s = torch.sum(f * x.unsqueeze(-2), dim=-2)\n",
    "        return s  \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        masked block self-attention module\n",
    "        :param x: (batch, seq_len, word_dim)\n",
    "        :param M: (seq_len, seq_len)\n",
    "        :return: (batch, seq_len, word_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.args.r\n",
    "        n = x.size(1)\n",
    "        m = n // r\n",
    "        \n",
    "        pad_len = (r - n % r) % r\n",
    "        if pad_len:\n",
    "            pad = Variable(torch.zeros(x.size(0),pad_len,x.size(2))).cuda(self.args.gpu).detach()\n",
    "            # pagging at sequence length\n",
    "            x = torch.cat([x,pad] , dim=2)\n",
    "         \n",
    "        # --- Intra-block self-attention ---\n",
    "        # (batch, block_num(m), seq_len(r), word_dim)\n",
    "        x = torch.stack([x.narrow(1,i,r) for i in range(0,x.size(1),r)],dim=1)\n",
    "        h = self.mSA(x)\n",
    "        v = self.s2tSA(h)\n",
    "        \n",
    "        o = self.mSA(v)\n",
    "        G = F.sigmoid(self.g_w1(0) + self.g_w2(v) + self.g_b)\n",
    "        e = G * O + (1-G)*V\n",
    "        \n",
    "        E = torch.cat([torch.stack([e.select(1, i)] * r, dim=1) for i in range(e.size(1))], dim=1).narrow(1, 0, n)\n",
    "        \n",
    "        # -1 代表自動推算\n",
    "        x = x.view(x.size(0),-1,x.size(-1)).narrow(1,0,n)\n",
    "        h = h.view(h.size(0), -1, h.size(-1)).narrow(1, 0, n)\n",
    "        \n",
    "        # fusion layer\n",
    "        fusion = self.f_W1(torch.cat([x, h, E], dim=2))\n",
    "        G = F.sigmoid(self.f_W2(torch.cat([x, h, E], dim=2)))\n",
    "        # (batch, n, word_dim)\n",
    "        u = G * fusion + (1 - G) * x\n",
    "\n",
    "        return u\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulid Biblosan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiBloSAN(customizedModule):\n",
    "    def __init__(self,args):\n",
    "        super(BiBloSAN,self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.mBloSA_fw = mBloSA(args,'fw')\n",
    "        self.mBloSA_bw = mBloSA(args,'bw')\n",
    "\n",
    "        # two untied fully connected layers\n",
    "        self.fc_fw = self.customizedLinear(self.args.word_dim, self.args.word_dim, activation=nn.ReLU())\n",
    "        self.fc_bw = self.customizedLinear(self.args.word_dim, self.args.word_dim, activation=nn.ReLU())\n",
    "\n",
    "        self.s2tSA = s2tSA(self.args, self.args.word_dim * 2)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        \n",
    "        ufw = self.fc_fw(x)      \n",
    "        ubw = self.fc_bw(x)\n",
    "        \n",
    "        ufw = self.mBloSA_fw(ufw) \n",
    "        ubw = self.fc_bw(ubw)\n",
    "        \n",
    "        # cat at word dimention\n",
    "        ubi = torch.cat([ufw,ubw],dim = -1)\n",
    "        s = self.s2tSA(ubi)\n",
    "        \n",
    "        return s\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
